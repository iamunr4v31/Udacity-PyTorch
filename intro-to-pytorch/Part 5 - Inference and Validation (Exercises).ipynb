{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Validation\n",
    "\n",
    "Now that you have a trained network, you can use it for making predictions. This is typically called **inference**, a term borrowed from statistics. However, neural networks have a tendency to perform *too well* on the training data and aren't able to generalize to data that hasn't been seen before. This is called **overfitting** and it impairs inference performance. To test for overfitting while training, we measure the performance on data not in the training set called the **validation** set. We avoid overfitting through regularization such as dropout while monitoring the validation performance during training. In this notebook, I'll show you how to do this in PyTorch. \n",
    "\n",
    "As usual, let's start by loading the dataset through torchvision. You'll learn more about torchvision and loading data in a later part. This time we'll be taking advantage of the test set which you can get by setting `train=False` here:\n",
    "\n",
    "```python\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "```\n",
    "\n",
    "The test set contains images just like the training set. Typically you'll see 10-20% of the original dataset held out for testing and validation with the rest being used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwi\\Anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll create a model like normal, using the same one from my solution for part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. Typically this is just accuracy, the percentage of classes the network predicted correctly. Other options are [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)) and top-5 error rate. We'll focus on accuracy here. First I'll do a forward pass with one batch from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "# Get the class probabilities\n",
    "ps = torch.exp(model(images))\n",
    "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the probabilities, we can get the most likely class using the `ps.topk` method. This returns the $k$ highest values. Since we just want the most likely class, we can use `ps.topk(1)`. This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [8],\n",
      "        [3],\n",
      "        [3],\n",
      "        [8],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Look at the most likely classes for the first 10 examples\n",
    "print(top_class[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if the predicted classes match the labels. This is simple to do by equating `top_class` and `labels`, but we have to be careful of the shapes. Here `top_class` is a 2D tensor with shape `(64, 1)` while `labels` is 1D with shape `(64)`. To get the equality to work out the way we want, `top_class` and `labels` must have the same shape.\n",
    "\n",
    "If we do\n",
    "\n",
    "```python\n",
    "equals = top_class == labels\n",
    "```\n",
    "\n",
    "`equals` will have shape `(64, 64)`, try it yourself. What it's doing is comparing the one element in each row of `top_class` with each element in `labels` which returns 64 True/False boolean values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals = top_class == labels.view(*top_class.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the percentage of correct predictions. `equals` has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to `torch.mean`. If only it was that simple. If you try `torch.mean(equals)`, you'll get an error\n",
    "\n",
    "```\n",
    "RuntimeError: mean is not implemented for type torch.ByteTensor\n",
    "```\n",
    "\n",
    "This happens because `equals` has type `torch.ByteTensor` but `torch.mean` isn't implemented for tensors with that type. So we'll need to convert `equals` to a float tensor. Note that when we take `torch.mean` it returns a scalar tensor, to get the actual value as a float we'll need to do `accuracy.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.375%\n"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is untrained so it's making random guesses and we should see an accuracy around 10%. Now let's train our network and include our validation pass so we can measure how well the network is performing on the test set. Since we're not updating our parameters in the validation pass, we can speed up our code by turning off gradients using `torch.no_grad()`:\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "```\n",
    "\n",
    ">**Exercise:** Implement the validation loop below and print out the total accuracy after the loop. You can largely copy and paste the code from above, but I suggest typing it in because writing it out yourself is essential for building the skill. In general you'll always learn more by typing it rather than copy-pasting. You should be able to get an accuracy above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] Accuracy: 0.8309 Train Loss: 0.0080907546967268 Test Loss: 0.0075851501896977425 Test Accuracy: 0.8309\n",
      "[2/30] Accuracy: 0.861 Train Loss: 0.006103724300364653 Test Loss: 0.006117320153862238 Test Accuracy: 0.861\n",
      "[3/30] Accuracy: 0.8575 Train Loss: 0.0054793433945626024 Test Loss: 0.006098234560340643 Test Accuracy: 0.8575\n",
      "[4/30] Accuracy: 0.8651 Train Loss: 0.005191305946186185 Test Loss: 0.005941068287938833 Test Accuracy: 0.8651\n",
      "[5/30] Accuracy: 0.868 Train Loss: 0.0049270317434022826 Test Loss: 0.005920384079217911 Test Accuracy: 0.868\n",
      "[6/30] Accuracy: 0.8683 Train Loss: 0.004672538242737452 Test Loss: 0.00586244510486722 Test Accuracy: 0.8683\n",
      "[7/30] Accuracy: 0.8733 Train Loss: 0.004573591641833385 Test Loss: 0.005534913390874863 Test Accuracy: 0.8733\n",
      "[8/30] Accuracy: 0.8667 Train Loss: 0.004366369575634598 Test Loss: 0.006048142444342375 Test Accuracy: 0.8667\n",
      "[9/30] Accuracy: 0.8712 Train Loss: 0.004268309483056267 Test Loss: 0.0057840957306325436 Test Accuracy: 0.8712\n",
      "[10/30] Accuracy: 0.8624 Train Loss: 0.004135173906385898 Test Loss: 0.006531946361064911 Test Accuracy: 0.8624\n",
      "[11/30] Accuracy: 0.8729 Train Loss: 0.004073256110586226 Test Loss: 0.005967922043055296 Test Accuracy: 0.8729\n",
      "[12/30] Accuracy: 0.8763 Train Loss: 0.003925615770618121 Test Loss: 0.005865253508090973 Test Accuracy: 0.8763\n",
      "[13/30] Accuracy: 0.8821 Train Loss: 0.0038530237581580877 Test Loss: 0.005773317534476519 Test Accuracy: 0.8821\n",
      "[14/30] Accuracy: 0.8776 Train Loss: 0.003774024114261071 Test Loss: 0.005748769734054804 Test Accuracy: 0.8776\n",
      "[15/30] Accuracy: 0.8735 Train Loss: 0.003674315157222251 Test Loss: 0.006524639669805765 Test Accuracy: 0.8735\n",
      "[16/30] Accuracy: 0.8813 Train Loss: 0.0036698937454571327 Test Loss: 0.005744731053709984 Test Accuracy: 0.8813\n",
      "[17/30] Accuracy: 0.8817 Train Loss: 0.003506649506216248 Test Loss: 0.005751801189035177 Test Accuracy: 0.8817\n",
      "[18/30] Accuracy: 0.8795 Train Loss: 0.0035082439434404172 Test Loss: 0.006155579350888729 Test Accuracy: 0.8795\n",
      "[19/30] Accuracy: 0.8845 Train Loss: 0.003443692972821494 Test Loss: 0.0060180616565048695 Test Accuracy: 0.8845\n",
      "[20/30] Accuracy: 0.8797 Train Loss: 0.003305265857403477 Test Loss: 0.006175768095999956 Test Accuracy: 0.8797\n",
      "[21/30] Accuracy: 0.8812 Train Loss: 0.0032489069307222963 Test Loss: 0.006187941413372755 Test Accuracy: 0.8812\n",
      "[22/30] Accuracy: 0.8846 Train Loss: 0.0032737404661253096 Test Loss: 0.006209813058376312 Test Accuracy: 0.8846\n",
      "[23/30] Accuracy: 0.8865 Train Loss: 0.0032032004823287327 Test Loss: 0.006179700139909983 Test Accuracy: 0.8865\n",
      "[24/30] Accuracy: 0.8846 Train Loss: 0.003139917261960606 Test Loss: 0.006475840695202351 Test Accuracy: 0.8846\n",
      "[25/30] Accuracy: 0.8778 Train Loss: 0.003048859103706976 Test Loss: 0.006458156742155552 Test Accuracy: 0.8778\n",
      "[26/30] Accuracy: 0.8807 Train Loss: 0.0030234303498019774 Test Loss: 0.006628625560551882 Test Accuracy: 0.8807\n",
      "[27/30] Accuracy: 0.8838 Train Loss: 0.002948389262581865 Test Loss: 0.006911174859851599 Test Accuracy: 0.8838\n",
      "[28/30] Accuracy: 0.8825 Train Loss: 0.0030334129226394 Test Loss: 0.007068726234138012 Test Accuracy: 0.8825\n",
      "[29/30] Accuracy: 0.8759 Train Loss: 0.0029669434585918985 Test Loss: 0.0065331412479281425 Test Accuracy: 0.8759\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.0028529346408322455 Test Loss: 0.006851700600236654 Test Accuracy: 0.8834\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        ## TODO: Implement the validation pass and print out the validation accuracy\n",
    "        tot_test_loss = 0\n",
    "        correct_test_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                tot_test_loss+=loss\n",
    "\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                correct_test_predictions+= equals.sum().item()\n",
    "            \n",
    "            test_loss = tot_test_loss/ len(testloader.dataset)\n",
    "            train_loss = running_loss/ len(trainloader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            accuracy = correct_test_predictions/ len(testloader.dataset)\n",
    "            print(\n",
    "                (f'[{e+1}/{epochs}] Accuracy: {accuracy}'),\n",
    "                (f\"Train Loss: {train_loss}\"),\n",
    "                (f\"Test Loss: {test_loss}\"),\n",
    "                (f\"Test Accuracy: {accuracy}\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting.\n",
    "\n",
    "<img src='assets/overfitting.png' width=450px>\n",
    "\n",
    "The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called *early-stopping*. In practice, you'd save the model frequently as you're training then later choose the model with the lowest validation loss.\n",
    "\n",
    "The most common method to reduce overfitting (outside of early-stopping) is *dropout*, where we randomly drop input units. This forces the network to share information between weights, increasing it's ability to generalize to new data. Adding dropout in PyTorch is straightforward using the [`nn.Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) module.\n",
    "\n",
    "```python\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use `model.eval()`. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with `model.train()`. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# set model back to train mode\n",
    "model.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Add dropout to your model and train it on Fashion-MNIST again. See if you can get a lower validation loss or higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define your model with dropout added\n",
    "class ClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  #flatten image\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.log_softmax(self.fc4(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/30] Accuracy: 0.8834 Train Loss: 0.012526162589093049 Test Loss: 0.011898641474545002 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.01140847936818997 Test Loss: 0.012382801622152328 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011247692646582921 Test Loss: 0.01165471225976944 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.010833579112092654 Test Loss: 0.011825107969343662 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011882184684773285 Test Loss: 0.011578282341361046 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011487780303259691 Test Loss: 0.011289292015135288 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011003239362935225 Test Loss: 0.012013399042189121 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.010826221664249897 Test Loss: 0.011758757755160332 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.01070122475475073 Test Loss: 0.010807547718286514 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011437461641430854 Test Loss: 0.011666331440210342 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011209593316912651 Test Loss: 0.01108876895159483 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011052343301475047 Test Loss: 0.011679650284349918 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011341835893193881 Test Loss: 0.011220930144190788 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.010814624892175198 Test Loss: 0.011025918647646904 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.01149663742085298 Test Loss: 0.013911483809351921 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.012135737949113051 Test Loss: 0.012595511972904205 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011891110639770826 Test Loss: 0.011955547146499157 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011260921018819014 Test Loss: 0.012947342358529568 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011449227657914162 Test Loss: 0.011571543291211128 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011163598143557708 Test Loss: 0.013132370077073574 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011155344271411499 Test Loss: 0.011223848909139633 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.01140493521640698 Test Loss: 0.011932415887713432 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.010933534651001294 Test Loss: 0.011063224636018276 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.01094719826777776 Test Loss: 0.01272923219949007 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011373742328584194 Test Loss: 0.011943105608224869 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.012053022981186708 Test Loss: 0.013199330307543278 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.012377077986796697 Test Loss: 0.012956684455275536 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011450399319827557 Test Loss: 0.011236104182898998 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011869647617141406 Test Loss: 0.012053110636770725 Test Accuracy: 0.8834\n",
      "[30/30] Accuracy: 0.8834 Train Loss: 0.011604003065824509 Test Loss: 0.013276583515107632 Test Accuracy: 0.8834\n"
     ]
    }
   ],
   "source": [
    "## TODO: Train your model with dropout, and monitor the training progress with the validation loss and accuracy\n",
    "criterion = nn.NLLLoss()\n",
    "model = ClassifierNetwork()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss+=loss.item()\n",
    "    else:\n",
    "        tot_test_loss = 0\n",
    "        test_corr = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                tot_test_loss+=loss\n",
    "\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                test_corr+=equals.sum().item()\n",
    "\n",
    "            test_loss = tot_test_loss/ len(testloader.dataset)\n",
    "            train_loss = running_loss/ len(trainloader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            accuracy = correct_test_predictions/ len(testloader.dataset)\n",
    "            print(\n",
    "                (f'[{epoch+1}/{epochs}] Accuracy: {accuracy}'),\n",
    "                (f\"Train Loss: {train_loss}\"),\n",
    "                (f\"Test Loss: {test_loss}\"),\n",
    "                (f\"Test Accuracy: {accuracy}\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model is trained, we can use it for inference. We've done this before, but now we need to remember to set the model in inference mode with `model.eval()`. You'll also want to turn off autograd with the `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import helper module (should be in the repo)\n",
    "import helper\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "\n",
    "ps = torch.exp(output)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.view(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up!\n",
    "\n",
    "In the next part, I'll show you how to save your trained models. In general, you won't want to train a model everytime you need it. Instead, you'll train once, save it, then load the model when you want to train more or use if for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
